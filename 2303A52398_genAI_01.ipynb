{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chsrinidh/GENERATIVE-AI-2025/blob/main/2303A52398_genAI_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''(1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actual\n",
        "values and deep learning model predicted values are shown in Table 1. Also compare the results\n",
        "with the outcomes of libraries\n",
        "YActual YP red\n",
        "20 20.5\n",
        "30 30.3\n",
        "40 40.2\n",
        "50 50.6\n",
        "60 60.7\n",
        "Tabela 1: YActual Vs. YP red'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OS_jPHcuRlrG",
        "outputId": "b67141ee-bfab-4307-9284-8622f0041769"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'(1 ponto) Write Python code from scratch to find error metrics of deep learning model. Actual\\nvalues and deep learning model predicted values are shown in Table 1. Also compare the results\\nwith the outcomes of libraries\\nYActual YP red\\n20 20.5\\n30 30.3\\n40 40.2\\n50 50.6\\n60 60.7\\nTabela 1: YActual Vs. YP red'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oVlHFKwK4D8",
        "outputId": "dbdaea3e-e1e7-49f8-f742-3d64f2d034ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Metrics Calculated From Scratch:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n",
            "\n",
            "Error Metrics Calculated Using Libraries:\n",
            "Mean Absolute Error (MAE): 0.4600000000000016\n",
            "Mean Squared Error (MSE): 0.24600000000000147\n",
            "Root Mean Squared Error (RMSE): 0.49598387070549127\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "Y_actual = [20, 30, 40, 50, 60]\n",
        "Y_pred = [20.5, 30.3, 40.2, 50.6, 60.7]\n",
        "\n",
        "def calculate_metrics_from_scratch(y_actual, y_pred):\n",
        "    n = len(y_actual)\n",
        "    absolute_errors = [abs(y_a - y_p) for y_a, y_p in zip(y_actual, y_pred)]\n",
        "    squared_errors = [(y_a - y_p) ** 2 for y_a, y_p in zip(y_actual, y_pred)]\n",
        "\n",
        "    mae = sum(absolute_errors) / n\n",
        "    mse = sum(squared_errors) / n\n",
        "    rmse = mse ** 0.5\n",
        "\n",
        "    return mae, mse, rmse\n",
        "\n",
        "mae_scratch, mse_scratch, rmse_scratch = calculate_metrics_from_scratch(Y_actual, Y_pred)\n",
        "\n",
        "mae_lib = mean_absolute_error(Y_actual, Y_pred)\n",
        "mse_lib = mean_squared_error(Y_actual, Y_pred)\n",
        "\n",
        "rmse_lib = np.sqrt(mse_lib)\n",
        "\n",
        "print(\"Error Metrics Calculated From Scratch:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_scratch}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_scratch}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_scratch}\")\n",
        "\n",
        "print(\"\\nError Metrics Calculated Using Libraries:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae_lib}\")\n",
        "print(f\"Mean Squared Error (MSE): {mse_lib}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse_lib}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.\n",
        "Actual values and deep learning model predicted values are shown in Table 2. Also compare the\n",
        "results with outcome of libraries\n",
        "YActual YP red\n",
        "0 0 1 1 2 0\n",
        "0 0 1 0 2 0\n",
        "0 1 1 2 2 1\n",
        "0 2 1 0 2 2\n",
        "0 2 1 2 2 2\n",
        "Tabela 2: YActual Vs. YP red\n",
        "• Expected Leaning Outcomes from this assignment related to python\n",
        "– Students are able to understand deep learning model metrics\n",
        "– Students are able to write code in python to find deep learning model metrics\n",
        "– Students are able to use python libraries to find deep learning model metrics\n",
        "• Naming cinvention\n",
        "– Report File Name: RollNo_Week No._Assignment No.'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "en_12SNrOGOd",
        "outputId": "71556ff7-ad57-4e4c-8274-0513eb6adfaf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1 ponto) Write python code from scratch to find evaluation metrics of deep learning model.\\nActual values and deep learning model predicted values are shown in Table 2. Also compare the\\nresults with outcome of libraries\\nYActual YP red\\n0 0 1 1 2 0\\n0 0 1 0 2 0\\n0 1 1 2 2 1\\n0 2 1 0 2 2\\n0 2 1 2 2 2\\nTabela 2: YActual Vs. YP red\\n• Expected Leaning Outcomes from this assignment related to python\\n– Students are able to understand deep learning model metrics\\n– Students are able to write code in python to find deep learning model metrics\\n– Students are able to use python libraries to find deep learning model metrics\\n• Naming cinvention\\n– Report File Name: RollNo_Week No._Assignment No.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "YActual = [0, 0, 0, 0, 0]\n",
        "YPred = [0, 0, 1, 2, 2]\n",
        "\n",
        "\n",
        "def calculate_metrics(y_actual, y_pred):\n",
        "    classes = np.unique(y_actual + y_pred)\n",
        "    tp = {cls: 0 for cls in classes}\n",
        "    fp = {cls: 0 for cls in classes}\n",
        "    fn = {cls: 0 for cls in classes}\n",
        "\n",
        "    for actual, pred in zip(y_actual, y_pred):\n",
        "        if actual == pred:\n",
        "            tp[actual] += 1\n",
        "        else:\n",
        "            fp[pred] += 1\n",
        "            fn[actual] += 1\n",
        "\n",
        "    metrics = {}\n",
        "    for cls in classes:\n",
        "        precision = tp[cls] / (tp[cls] + fp[cls]) if (tp[cls] + fp[cls]) > 0 else 0\n",
        "        recall = tp[cls] / (tp[cls] + fn[cls]) if (tp[cls] + fn[cls]) > 0 else 0\n",
        "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        metrics[cls] = {\"Precision\": precision, \"Recall\": recall, \"F1 Score\": f1}\n",
        "\n",
        "    accuracy = sum(tp.values()) / len(y_actual)\n",
        "    return accuracy, metrics\n",
        "\n",
        "\n",
        "accuracy_manual, metrics_manual = calculate_metrics(YActual, YPred)\n",
        "\n",
        "\n",
        "accuracy_lib = accuracy_score(YActual, YPred)\n",
        "precision_lib = precision_score(YActual, YPred, average=None, labels=np.unique(YActual))\n",
        "recall_lib = recall_score(YActual, YPred, average=None, labels=np.unique(YActual))\n",
        "f1_lib = f1_score(YActual, YPred, average=None, labels=np.unique(YActual))\n",
        "report_lib = classification_report(YActual, YPred, zero_division=0)\n",
        "\n",
        "\n",
        "print(\"Manual Calculation:\")\n",
        "print(f\"Accuracy: {accuracy_manual}\")\n",
        "for cls, metrics in metrics_manual.items():\n",
        "    print(f\"Class {cls}: Precision: {metrics['Precision']}, Recall: {metrics['Recall']}, F1 Score: {metrics['F1 Score']}\")\n",
        "\n",
        "print(\"\\nUsing Libraries:\")\n",
        "print(f\"Accuracy: {accuracy_lib}\")\n",
        "for idx, cls in enumerate(np.unique(YActual)):\n",
        "    print(f\"Class {cls}: Precision: {precision_lib[idx]}, Recall: {recall_lib[idx]}, F1 Score: {f1_lib[idx]}\")\n",
        "print(\"\\nDetailed Report:\\n\", report_lib)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4stZQ08IVyPD",
        "outputId": "1b05912b-2c62-4648-d02e-44afd2fa4215"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manual Calculation:\n",
            "Accuracy: 0.4\n",
            "Class 0: Precision: 1.0, Recall: 0.4, F1 Score: 0.5714285714285715\n",
            "Class 1: Precision: 0.0, Recall: 0, F1 Score: 0\n",
            "Class 2: Precision: 0.0, Recall: 0, F1 Score: 0\n",
            "\n",
            "Using Libraries:\n",
            "Accuracy: 0.4\n",
            "Class 0: Precision: 1.0, Recall: 0.4, F1 Score: 0.5714285714285714\n",
            "\n",
            "Detailed Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.40      0.57         5\n",
            "           1       0.00      0.00      0.00         0\n",
            "           2       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           0.40         5\n",
            "   macro avg       0.33      0.13      0.19         5\n",
            "weighted avg       1.00      0.40      0.57         5\n",
            "\n"
          ]
        }
      ]
    }
  ]
}